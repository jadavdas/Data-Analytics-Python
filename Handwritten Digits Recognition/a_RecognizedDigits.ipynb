{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "svc = svm.SVC(gamma=0.001, C=100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n"
     ]
    }
   ],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1797"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, ..., 8, 9, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d8e79d9ef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(digits.data[1:1790], digits.target[1:1790])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2d8e7acb550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.imshow(digits.images[1791], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.subplot(322)\n",
    "plt.imshow(digits.images[1792], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.subplot(323)\n",
    "plt.imshow(digits.images[1793], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.subplot(324)\n",
    "plt.imshow(digits.images[1794], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.subplot(325)\n",
    "plt.imshow(digits.images[1795], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.subplot(326)\n",
    "plt.imshow(digits.images[1796], cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 9, 0, 8, 9, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.predict(digits.data[1791:1976])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 9, 0, 8, 9, 8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.target[1791:1976]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\jadav das\\non project\\python\\jupyter\\venv\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-20e788d9734d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpixels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"list of values loaded \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreal_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "pixels, real_values = train_images(10)\n",
    "print(\"list of values loaded \", real_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'disable_v2_behavior'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1131e86d0163>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_v2_behavior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'disable_v2_behavior'"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# tf Graph Input\n",
    "x = tf.placeholder(\"float\", [None, 784]) # mnist data image of shape 28*28=784\n",
    "y = tf.placeholder(\"float\", [None, 10]) # 0-9 digits recognition => 10 classes\n",
    "\n",
    "# Create model\n",
    "\n",
    "# Set model weights\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "evidence = tf.matmul(x, W) + b\n",
    "\n",
    "# Construct model\n",
    "activation = tf.nn.softmax(evidence) # Softmax\n",
    "\n",
    "# Minimize error using cross entropy\n",
    "cross_entropy = y*tf.log(activation)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(cross_entropy,reduction_indices=1))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) \n",
    "\n",
    "#Plot settings\n",
    "avg_set = []\n",
    "epoch_set=[]\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(train_images/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = train_images(batch_size)\n",
    "            sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n",
    "            avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "        avg_set.append(avg_cost)\n",
    "        epoch_set.append(epoch+1)\n",
    "    print(\"Training phase finished\")\n",
    "\n",
    "    plt.plot(epoch_set,avg_set, 'o', label='Logistic Regression Training phase')\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(activation, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Model accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
